{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786abfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\transformers\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e46fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f3399b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370fa0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b95c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bf99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ds(examples):\n",
    "    return tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5eee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds[\"train\"].map(preprocess_ds, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8535c366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'brussels', '1996', '-', '08', '-', '22', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(test[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08beeef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ad0d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f53c1a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BRUSSELS', '1996-08-22']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de4449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"] = ds[\"train\"].select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b1748a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc81063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ds(examples):\n",
    "    tokenized_input = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    final_labels = list()\n",
    "    for idx, _ in enumerate(tokenized_input[\"input_ids\"]):\n",
    "        word_ids = tokenized_input.word_ids(batch_index=idx)\n",
    "        current_label = list()\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                current_label.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                previous_word_id = word_id\n",
    "                current_ner_tag = examples[\"ner_tags\"][idx]\n",
    "                current_label.append(current_ner_tag[word_id])\n",
    "            else:\n",
    "                current_label.append(-100)\n",
    "        final_labels.append(current_label)\n",
    "    # print(final_labels)\n",
    "    # print(tokenized_input[\"input_ids\"])\n",
    "    assert len(final_labels) == len(tokenized_input[\"input_ids\"])\n",
    "    tokenized_input[\"labels\"] = final_labels\n",
    "     \n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa0aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds[\"train\"].map(preprocess_ds, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a495f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6488a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a382fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(preprocess_ds, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f21d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14041"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff942d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14041"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[\"train\"][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d3221b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ds[\"train\"].features[\"ner_tags\"].feature.names\n",
    "id2label = {idx: name for idx, name in enumerate(class_names)}\n",
    "num_labels = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9fc2557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4934ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fef35bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8dea35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(output):\n",
    "    predictions, labels = output\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    new_predictions = [[class_names[p] for p, l in zip(prediction, label) if l!=-100] for prediction, label in zip(predictions, labels)]\n",
    "    new_labels = [[class_names[l] for p, l in zip(prediction, label) if l!=-100] for prediction, label in zip(predictions, labels)]\n",
    "    results = seqeval.compute(predictions=new_predictions, references=new_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for Hugging Face Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",                     # Directory where checkpoints, logs, etc. are saved\n",
    "    per_device_train_batch_size=16,        # Training batch size per GPU/CPU\n",
    "    per_device_eval_batch_size=16,         # Evaluation batch size per GPU/CPU\n",
    "    num_train_epochs=3,                    # Number of training epochs\n",
    "    eval_strategy=\"epoch\",                 # Run evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save model checkpoint at the end of each epoch\n",
    "    logging_strategy=\"epoch\",              # Log training metrics (loss, etc.) once per epoch\n",
    "    metric_for_best_model=\"eval_loss\",     # Metric used to decide the \"best model\"\n",
    "    save_total_limit=1,                    # Keep only the most recent checkpoint (older ones deleted)\n",
    "    load_best_model_at_end=True,           # Load best model at the end (set True if you want best model)\n",
    "    push_to_hub=False,                     # Push model to Hugging Face Hub\n",
    "    fp16=False,                             # Use mixed precision (float16) for faster training on GPUs\n",
    "    gradient_accumulation_steps=1,         # Accumulate gradients for 4 steps before backward/update -> Effective batch size = 16 * 4 = 64\n",
    "    # lr_scheduler_type=\"cosine\",            # Use cosine learning rate scheduler\n",
    "    # report_to=\"mlflow\"                     # Report logs & metrics to MLflow\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    args=training_args,                    # Training arguments defined above\n",
    "    model=model,                           # Model to train (BERT sequence classification in this case)\n",
    "    train_dataset=ds[\"train\"],             # Training dataset\n",
    "    eval_dataset=ds[\"test\"],               # Evaluation dataset\n",
    "    data_collator=datacollator,            # Function to batch and pad inputs\n",
    "    processing_class=tokenizer,            # Alternate for datacollator with padding\n",
    "    compute_metrics=compute_metrics,       # Function to compute custom metrics (accuracy, F1, etc.)\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # Stop training if no improvement for 3 evaluation rounds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6fa141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 02:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.098922</td>\n",
       "      <td>0.879958</td>\n",
       "      <td>0.899433</td>\n",
       "      <td>0.889589</td>\n",
       "      <td>0.977280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.109452</td>\n",
       "      <td>0.896832</td>\n",
       "      <td>0.917316</td>\n",
       "      <td>0.906958</td>\n",
       "      <td>0.980575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.125107</td>\n",
       "      <td>0.903412</td>\n",
       "      <td>0.914129</td>\n",
       "      <td>0.908739</td>\n",
       "      <td>0.980876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\admin\\anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.04708535506495401, metrics={'train_runtime': 160.8641, 'train_samples_per_second': 261.855, 'train_steps_per_second': 16.374, 'total_flos': 1020143109346326.0, 'train_loss': 0.04708535506495401, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e9fbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29143366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "c:\\Users\\admin\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"ner\", \"best_model\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0f54b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.99209166),\n",
       "  'word': 'vijay',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.96222115),\n",
       "  'word': 'tamilnadu',\n",
       "  'start': 24,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier(\"Vijay is going to cm of tamilnadu\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c2a7bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vijay\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " is going to cm of \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tamilnadu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Create a spaCy-style Doc for visualization\n",
    "doc = {\n",
    "    \"text\": \"Vijay is going to cm of tamilnadu\",\n",
    "    \"ents\": [\n",
    "        {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "        for ent in results\n",
    "    ],\n",
    "    \"title\": None\n",
    "}\n",
    "\n",
    "# 3. Use displaCy to render in Jupyter/Colab\n",
    "displacy.render(doc, style=\"ent\", manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b7bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
